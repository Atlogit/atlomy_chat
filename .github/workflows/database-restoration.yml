name: Database Restoration Workflow

on:
  workflow_dispatch:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - development
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        required: true
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: true
        default: 'amta-db'
  workflow_call:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        type: string
        required: false
        default: 'production'
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        type: string
        required: false
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: false
        default: 'amta-db'

env:
  # Use workflow input for PostgreSQL data directory
  POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
  # Use workflow input for database backups directory
  DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

permissions:
  id-token: write
  contents: read

jobs:
  database-restoration:
    runs-on: ubuntu-latest
    environment: ${{ inputs.deployment_mode }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Validate Paths
        run: |
          echo "üìç PostgreSQL Data Directory: $POSTGRES_DATA_DIR"
          echo "üì¶ Database Backup Directory: $DATABASE_BACKUP_DIR"
          
          # Validate paths are not empty
          if [ -z "$POSTGRES_DATA_DIR" ]; then
            echo "‚ùå POSTGRES_DATA_DIR must be set"
            exit 1
          fi
          
          if [ -z "$DATABASE_BACKUP_DIR" ]; then
            echo "‚ùå DATABASE_BACKUP_DIR must be set"
            exit 1
          fi

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          if ! secrets=$(aws secretsmanager get-secret-value --secret-id amta-${{ inputs.deployment_mode }}-secrets --query SecretString --output text); then
            echo "Failed to retrieve secrets from AWS Secrets Manager"
            exit 1
          fi
          
          # Parse database connection details
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          if [ -z "$database_url" ]; then
            echo "DATABASE_URL not found in secrets"
            exit 1
          fi
          
          # Extract database connection parameters
          postgres_host=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@([^:/]+).*|\1|')
          postgres_port=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?([0-9]+)?.*|\1|' || echo "5432")
          postgres_db=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?[0-9]*/([^?]+).*|\1|')
          postgres_user=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://([^:]+):.*|\1|')
          postgres_password=$(echo "$secrets" | jq -r '.POSTGRES_PASSWORD')
          
          # Validate extracted values
          for var in postgres_host postgres_db postgres_user postgres_password; do
            if [ -z "${!var}" ]; then
              echo "Failed to extract $var"
              exit 1
            fi
          done

          # Set outputs
          {
            echo "POSTGRES_HOST=$postgres_host"
            echo "POSTGRES_PORT=$postgres_port"
            echo "POSTGRES_DB=$postgres_db"
            echo "POSTGRES_USER=$postgres_user"
            echo "POSTGRES_PASSWORD=$postgres_password"
          } >> "$GITHUB_OUTPUT"

      - name: Prepare SSH Key
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
        run: |
          if [ -z "$EC2_SSH_KEY" ]; then
            echo "EC2 SSH key is not set"
            exit 1
          fi
          
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key

      - name: Check Database Initialization Status
        id: check-database
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Check if PostgreSQL data directory exists and contains PG_VERSION
          if ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
             "[ ! -f '$POSTGRES_DATA_DIR/PG_VERSION' ]"; then
            echo "Database not initialized. Restoration required."
            echo "restore_needed=true" >> $GITHUB_OUTPUT
          else
            echo "Database directory exists."
            echo "restore_needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Discover Latest S3 Backup
        id: discover-backup
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
        run: |
          # List and sort backups by date, get the latest
          latest_backup=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/" \
            | grep '\.tar\.gz$' \
            | sort -k1,2 \
            | tail -n 1 \
            | awk '{print $4}')
          
          if [ -z "$latest_backup" ]; then
            echo "‚ùå No backup files found in S3"
            exit 1
          fi

          # Get backup timestamp
          backup_timestamp=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$latest_backup" | awk '{print $1 " " $2}')
          
          echo "Latest backup: $latest_backup"
          echo "Backup timestamp: $backup_timestamp"
          echo "latest_backup=$latest_backup" >> $GITHUB_OUTPUT
          echo "backup_timestamp=$backup_timestamp" >> $GITHUB_OUTPUT

      - name: Compare Backup Timestamp
        id: compare-timestamp
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          BACKUP_TIMESTAMP: ${{ steps.discover-backup.outputs.backup_timestamp }}
          CHECK_DATABASE_RESTORE_NEEDED: ${{ steps.check-database.outputs.restore_needed }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # If database is already marked for restoration, skip timestamp comparison
          if [ "$CHECK_DATABASE_RESTORE_NEEDED" = "true" ]; then
            echo "Database already marked for restoration from previous check."
            echo "restore_needed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Perform timestamp comparison only if database exists
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
            
            # Get local database timestamp
            local_timestamp=$(stat -c %y "$POSTGRES_DATA_DIR/PG_VERSION")
            
            # Convert timestamps to seconds since epoch
            backup_seconds=$(date -d "$BACKUP_TIMESTAMP" +%s)
            local_seconds=$(date -d "$local_timestamp" +%s)
            
            # Compare timestamps
            if [ "$backup_seconds" -gt "$local_seconds" ] || [ "${{ inputs.force_restore }}" = "true" ]; then
              echo "Backup is newer than local database or force restore requested."
              echo "restore_needed=true" >> $GITHUB_OUTPUT
            else
              echo "Local database is up to date."
              echo "restore_needed=false" >> $GITHUB_OUTPUT
            fi
          REMOTE_SCRIPT

      - name: Initialize PostgreSQL Data Directory
        id: init-postgres
        if: steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
      
        run: |

          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Verify environment variables
          for var in POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD EC2_HOST EC2_USER; do
            if [ -z "${!var}" ]; then
              echo "Error: $var is not set"
              exit 1
            fi
          done

          # Execute remote initialization
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
                  
            # Export variables for use in the script
            export EC2_USER="$EC2_USER"
            export POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR"
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"
      
            # Define directories using the current user
            echo "üîÑ Starting PostgreSQL initialization in $POSTGRES_DATA_DIR..."
            
            # Create postgres user and group with specific UID/GID (matching Docker postgres image)
            sudo groupadd -f -g 999 postgres || true
            if ! id postgres &>/dev/null; then
              sudo useradd -r -u 999 -g postgres postgres || true
            fi

            # Create parent directories with EC2 user ownership
            sudo mkdir -p "\$(dirname \$POSTGRES_DATA_DIR)"
            sudo mkdir -p "\$(dirname \$DATABASE_BACKUP_DIR)"
            sudo chown -R "\$EC2_USER:\$EC2_USER" "\$(dirname \$POSTGRES_DATA_DIR)"
            sudo chown -R "\$EC2_USER:\$EC2_USER" "\$(dirname \$DATABASE_BACKUP_DIR)"
            
            # Create backup directory with EC2 user ownership
            mkdir -p "\$DATABASE_BACKUP_DIR"
            chmod 755 "\$DATABASE_BACKUP_DIR"
            
            # Clean and prepare PostgreSQL data directory
            sudo rm -rf "\$POSTGRES_DATA_DIR"
            sudo mkdir -p "\$POSTGRES_DATA_DIR"
            sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
            sudo chmod 0777 "\$POSTGRES_DATA_DIR"  # Temporarily more permissive

            # Backup existing data if present (as EC2 user)
            if [ -d "\$POSTGRES_DATA_DIR" ] && [ "\$(sudo ls -A \$POSTGRES_DATA_DIR)" ]; then
              timestamp=\$(date +%Y%m%d_%H%M%S)
              echo "üì¶ Backing up existing data..."
              sudo tar czf "\${DATABASE_BACKUP_DIR}/pg_data_backup_\${timestamp}.tar.gz" \
                -C "\$(dirname \$POSTGRES_DATA_DIR)" "\$(basename \$POSTGRES_DATA_DIR)"
              sudo rm -rf "\$POSTGRES_DATA_DIR"/*
            fi

            # Verify Docker is available
            if ! command -v docker &> /dev/null; then
              echo "‚ùå Docker is not installed"
              exit 1
            fi

            # Run Docker initialization
            echo "üê≥ Initializing PostgreSQL using Docker..."
            sudo docker run --rm \
              -v "\$POSTGRES_DATA_DIR:\$POSTGRES_DATA_DIR" \
              -e POSTGRES_DB="$POSTGRES_DB" \
              -e POSTGRES_USER="$POSTGRES_USER" \
              -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
              --user 999:999 \
              postgres:14 \
              initdb -D "\$POSTGRES_DATA_DIR"
            
            # Ensure final permissions are correct
            sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
            sudo chmod 0700 "\$POSTGRES_DATA_DIR"
            
            echo "‚úÖ PostgreSQL initialization completed successfully"
          REMOTE_SCRIPT

      - name: Download Latest Backup
        id: download-backup
        if: steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
        run: |
          echo "üîç Downloading backup: $LATEST_BACKUP"
          
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Transfer and verify backup
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
            
            # Ensure backup directory exists
            sudo mkdir -p "$DATABASE_BACKUP_DIR"
            sudo chown -R $EC2_USER:$EC2_USER "$DATABASE_BACKUP_DIR"
            
            echo "‚¨áÔ∏è Downloading backup from S3..."
            aws s3 cp "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$LATEST_BACKUP" \
              "$DATABASE_BACKUP_DIR/latest_backup.tar.gz"
            
            if [ ! -f "$DATABASE_BACKUP_DIR/latest_backup.tar.gz" ]; then
              echo "‚ùå Backup download failed"
              exit 1
            fi
            
            # Verify backup integrity and structure
            backup_contents=$(tar tzf $DATABASE_BACKUP_DIR/latest_backup.tar.gz)
            
            # Check if the backup contains a PostgreSQL data directory
            if ! echo "$backup_contents" | grep -q "PG_VERSION"; then
              echo "‚ùå Backup does not appear to be a valid PostgreSQL data directory"
              exit 1
            fi
            
            # Additional checks for PostgreSQL directory structure
            if ! echo "$backup_contents" | grep -q "base/"; then
              echo "‚ùå Backup is missing essential PostgreSQL data subdirectories"
              exit 1
            fi
            
            echo "‚úÖ Backup downloaded and verified successfully"
          REMOTE_SCRIPT

          # Verify initialization

      - name: Restore Database
        id: restore-database
        if: steps.download-backup.outcome == 'success'
        env:
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
      
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e
            
            # Export variables for use in the script
            export EC2_USER="$EC2_USER"
            export POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR"
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"
            export PGPASSWORD="$POSTGRES_PASSWORD"
            
            echo "üîÑ Starting database restoration process..."
            
            # Ensure PostgreSQL is stopped before restoration
            sudo systemctl stop postgresql || true
                        
            # Create temporary directory for extraction
            TEMP_DIR=$(mktemp -d)
            trap 'sudo rm -rf "\$TEMP_DIR"' EXIT
            
            # Extract backup
            echo "ÔøΩ Extracting backup..."
            sudo tar xzf "\$DATABASE_BACKUP_DIR/latest_backup.tar.gz" -C "\$TEMP_DIR"
            
            # Determine backup type
            DUMP_FILE=\$(sudo find "\$TEMP_DIR" -name "*.dump" -type f)
            PG_VERSION_FILE=\$(sudo find "\$TEMP_DIR" -name "PG_VERSION" -type f)
            
            if [ -n "\$PG_VERSION_FILE" ]; then
              echo "üóÇÔ∏è Full PostgreSQL data directory backup detected"
              
              # Copy entire data directory
              sudo cp -R "\$TEMP_DIR"/* "\$POSTGRES_DATA_DIR"
              # Ensure correct ownership and permissions
              sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
              sudo chmod 700 "\$POSTGRES_DATA_DIR"     
              
              # Verify PostgreSQL data directory
              if [ ! -f "\$POSTGRES_DATA_DIR/PG_VERSION" ]; then
                echo "‚ùå PostgreSQL data directory restoration failed"
                exit 1
              fi
                
            elif [ -n "\$DUMP_FILE" ]; then
              echo "üìÑ Database dump file detected"
              
              # Start PostgreSQL with the initialized directory
              sudo systemctl start postgresql
              sleep 5  # Wait for PostgreSQL to start

              # Drop existing connections
              psql -h "$POSTGRES_HOST" -p "$POSTGRES_PORT" -U "$POSTGRES_USER" -d postgres -c \
              "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB' AND pid <> pg_backend_pid();"
              
              # Restore the database from dump
              pg_restore --clean --if-exists --no-owner --no-privileges \
              -h "$POSTGRES_HOST" -p "$POSTGRES_PORT" -U "$POSTGRES_USER" -d "$POSTGRES_DB" "\$DUMP_FILE"
                  
              # Verify restoration
              if ! psql -h "$POSTGRES_HOST" -p "$POSTGRES_PORT" -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c \
                "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" | grep -q '[1-9]'; then
                echo "‚ùå Database restoration from dump failed or resulted in empty database"
                exit 1
              fi
                
            else
              echo "‚ùå No valid backup found: neither PostgreSQL data directory nor dump file detected"
              exit 1
            fi
            
            # Restart PostgreSQL to apply changes
            sudo systemctl restart postgresql
            sleep 5  # Wait for PostgreSQL to restart
                              
            # Verify database connectivity
            if ! psql -h "$POSTGRES_HOST" -p "$POSTGRES_PORT" -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT 1;" &>/dev/null; then
              echo "‚ùå Database is not responding after restoration"
            else
              echo "‚úÖ Database restored successfully"
              echo "restore_status=success" >> $GITHUB_OUTPUT
            fi
          REMOTE_SCRIPT

  notify-status:
    if: always()
    needs: [database-restoration]
    runs-on: ubuntu-latest
    steps:
      - name: Determine Workflow Status
        run: |
          if [[ "${{ needs.database-restoration.result }}" == 'success' ]]; then
            echo "‚úÖ Database Restoration Completed Successfully"
          else
            echo "‚ùå Database Restoration Failed"
            exit 1
          fi
