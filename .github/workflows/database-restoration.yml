name: Database Restoration Workflow

on:
  workflow_dispatch:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - development
      backup_type:
        description: 'Backup Type'
        required: true
        type: choice
        options:
          - tar
          - sql
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        required: true
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: true
        default: 'amta-db'
  workflow_call:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        type: string
        required: false
        default: 'production'
      backup_type:
        description: 'Backup Type'
        type: string
        required: true
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        type: string
        required: false
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: false
        default: 'amta-db'

env:
  # Use workflow input for PostgreSQL data directory
  POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
  # Use workflow input for database backups directory
  DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

permissions:
  id-token: write
  contents: read

jobs:
  database-restoration:
    runs-on: ubuntu-latest
    environment: ${{ inputs.deployment_mode }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Validate Paths
        run: |
          echo "📍 PostgreSQL Data Directory: $POSTGRES_DATA_DIR"
          echo "📦 Database Backup Directory: $DATABASE_BACKUP_DIR"
          
          # Validate paths are not empty
          if [ -z "$POSTGRES_DATA_DIR" ]; then
            echo "❌ POSTGRES_DATA_DIR must be set"
            exit 1
          fi
          
          if [ -z "$DATABASE_BACKUP_DIR" ]; then
            echo "❌ DATABASE_BACKUP_DIR must be set"
            exit 1
          fi

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          if ! secrets=$(aws secretsmanager get-secret-value --secret-id amta-${{ inputs.deployment_mode }}-secrets --query SecretString --output text); then
            echo "Failed to retrieve secrets from AWS Secrets Manager"
            exit 1
          fi
          
          # Parse database connection details
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          if [ -z "$database_url" ]; then
            echo "DATABASE_URL not found in secrets"
            exit 1
          fi
          
          # Extract database connection parameters
          postgres_host=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@([^:/]+).*|\1|')
          postgres_db=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?[0-9]*/([^?]+).*|\1|')
          postgres_user=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://([^:]+):.*|\1|')
          postgres_password=$(echo "$secrets" | jq -r '.POSTGRES_PASSWORD')
          postgres_port=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?([0-9]+)?/.*|\1|')
          if [ -z "$postgres_port" ]; then
              postgres_port="5432"
          fi
          # Validate extracted values
          for var in postgres_host postgres_db postgres_user postgres_password; do
            if [ -z "${!var}" ]; then
              echo "Failed to extract $var"
              exit 1
            fi
          done

          # Set outputs
          {
            echo "POSTGRES_HOST=$postgres_host"
            echo "POSTGRES_PORT=$postgres_port"
            echo "POSTGRES_DB=$postgres_db"
            echo "POSTGRES_USER=$postgres_user"
            echo "POSTGRES_PASSWORD=$postgres_password"
          } >> "$GITHUB_OUTPUT"

      - name: Prepare SSH Key
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
        run: |
          if [ -z "$EC2_SSH_KEY" ]; then
            echo "EC2 SSH key is not set"
            exit 1
          fi
          
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key

      - name: Check Database Initialization Status
        id: check-database
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Pass environment variables directly to SSH command
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
            "POSTGRES_DATA_DIR='$POSTGRES_DATA_DIR' \
             DATABASE_BACKUP_DIR='$DATABASE_BACKUP_DIR' \
             POSTGRES_DB='$POSTGRES_DB' \
             POSTGRES_USER='$POSTGRES_USER' \
             POSTGRES_PASSWORD='$POSTGRES_PASSWORD' \
             bash -s" << 'REMOTE_SCRIPT'
            set -e

            echo "Debug: Environment variables:"
            echo "POSTGRES_DATA_DIR=$POSTGRES_DATA_DIR"
            echo "DATABASE_BACKUP_DIR=$DATABASE_BACKUP_DIR"
            echo "POSTGRES_DB=$POSTGRES_DB"
            echo "POSTGRES_USER=$POSTGRES_USER"

            # Function to check PostgreSQL initialization status
            check_postgres_initialization() {
              local data_dir="$POSTGRES_DATA_DIR"
              local status="UNINITIALIZED"
              
              echo "🔍 Checking PostgreSQL data directory: $data_dir"
              
              # Debug: Show directory contents and permissions
              echo "Directory listing:"
              sudo ls -la "$data_dir" || echo "Directory does not exist or not accessible"
              
              # Debug: Try to find PG_VERSION
              echo "Searching for PG_VERSION:"
              sudo find "$data_dir" -name PG_VERSION 2>/dev/null || echo "PG_VERSION not found"
              
              # Step 1: Check if directory exists and has basic structure
              if sudo test -d "$data_dir" && sudo test -f "$data_dir/PG_VERSION"; then
                echo "Found PG_VERSION file"
                
                # Check for essential PostgreSQL directories
                local required_dirs=("base" "global" "pg_wal" "pg_xact")
                local missing_dirs=0
                
                for dir in "${required_dirs[@]}"; do
                  echo "Checking directory: $dir"
                  if ! sudo test -d "$data_dir/$dir"; then
                    echo "Missing directory: $dir"
                    missing_dirs=$((missing_dirs + 1))
                  else
                    echo "Found directory: $dir"
                  fi
                done
                
                if [ $missing_dirs -eq 0 ]; then
                  echo "All required directories present"
                  status="INITIALIZED"
                else
                  echo "Missing $missing_dirs required directories"
                  status="CORRUPTED"
                fi
                
                # Step 2: If initialized, verify with temporary container
                if [ "$status" = "INITIALIZED" ]; then
                  echo "Verifying PostgreSQL data with temporary container..."
                  
                  # Clean up any existing containers
                  docker ps -q --filter "name=pg-verify" | xargs -r docker stop
                  docker ps -aq --filter "name=pg-verify" | xargs -r docker rm
                  
                  # Ensure proper permissions for Docker
                  echo "Setting proper permissions for Docker..."
                  if [ -d "$data_dir" ]; then
                    sudo find "$data_dir" -type f -exec sudo chown 999:999 {} \;
                    sudo find "$data_dir" -type d -exec sudo chown 999:999 {} \;
                    sudo find "$data_dir" -type f -exec sudo chmod 0600 {} \;
                    sudo find "$data_dir" -type d -exec sudo chmod 0700 {} \;
                  fi
                  
                  # Start PostgreSQL with the existing data directory
                  if ! docker run -d --rm \
                    --name pg-verify \
                    -v "$data_dir:/var/lib/postgresql/data" \
                    -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                    -e POSTGRES_USER="$POSTGRES_USER" \
                    -e POSTGRES_DB="$POSTGRES_DB" \
                    --user 999:999 \
                    postgres:14; then
                    echo "Container failed to start"
                    status="CORRUPTED"
                  else
                    # Wait for PostgreSQL to be ready
                    for i in {1..30}; do
                      if docker exec pg-verify pg_isready -U "$POSTGRES_USER" &>/dev/null; then
                        echo "Container verification successful"
                        docker stop pg-verify
                        break
                      fi
                      if [ $i -eq 30 ]; then
                        echo "Container verification failed"
                        docker stop pg-verify
                        status="CORRUPTED"
                      fi
                      sleep 1
                    done
                  fi
                fi
              fi
              
              echo "Final status: $status"
              echo "STATUS=$status"
              return 0
            }
            
            # Main execution
            echo "Starting database initialization check..."
            status=$(check_postgres_initialization)
            echo "Check completed with status: $status"
            actual_status=$(echo "$status" | grep "STATUS=" | cut -d'=' -f2)

            # Set status code based on result
            case "$actual_status" in
              "UNINITIALIZED")
                echo "Database is not initialized"
                db_status=2
                ;;
              "CORRUPTED")
                echo "Database is corrupted or incomplete"
                db_status=3
                ;;
              "INITIALIZED")
                echo "Database is properly initialized"
                db_status=0
                ;;
              *)
                echo "Unknown database status"
                db_status=1
                ;;
            esac
            
            # Output final status and ensure we don't exit
            echo "DB_STATUS=$db_status"
          REMOTE_SCRIPT
          )
          
          # Print output for logging
          echo "Check output: $CHECK_OUTPUT"
          
          # Extract the status code from the output
          if echo "$CHECK_OUTPUT" | grep -q "DB_STATUS="; then
            db_status=$(echo "$CHECK_OUTPUT" | grep "DB_STATUS=" | cut -d'=' -f2)
          else
            db_status=1
          fi
                    
          # Set GitHub outputs based on status
          case $db_status in
            0)
              echo "status=initialized" >> $GITHUB_OUTPUT
              echo "restore_needed=false" >> $GITHUB_OUTPUT
              ;;
            2)
              echo "status=uninitialized" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
            3)
              echo "status=corrupted" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "status=unknown" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
          esac
          
      - name: Discover Latest S3 Backup
        id: discover-backup
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
          BACKUP_TYPE: ${{ inputs.backup_type }}
        run: |
          # Determine file extension based on backup type
          if [ "$BACKUP_TYPE" == "tar" ]; then
            file_pattern='\.tar\.gz$'
          elif [ "$BACKUP_TYPE" == "sql" ]; then
            file_pattern='\.sql\.gz$'
          else
            echo "Invalid backup type"
            exit 1
          fi

          latest_backup=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/" \
            | grep "$file_pattern" \
            | sort -k1,2 \
            | tail -n 1 \
            | awk '{print $4}')

          if [ -z "$latest_backup" ]; then
            echo "❌ No backup files found in S3"
            exit 1
          fi

          backup_timestamp=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$latest_backup" | awk '{print $1 " " $2}')
          
          echo "Latest backup: $latest_backup"
          echo "Backup timestamp: $backup_timestamp"
          echo "backup_file=${latest_backup}" >> "$GITHUB_OUTPUT"
          echo "backup_timestamp=${backup_timestamp}" >> "$GITHUB_OUTPUT"
      
      - name: Compare Backup Timestamp
        id: compare-timestamp
        if: steps.check-database.outputs.status == 'initialized'
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.backup_file }}
          BACKUP_TIMESTAMP: ${{ steps.discover-backup.outputs.backup_timestamp }}
          CHECK_DATABASE_RESTORE_NEEDED: ${{ steps.check-database.outputs.restore_needed }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          echo "Starting timestamp comparison..."
          echo "Backup timestamp from S3: $BACKUP_TIMESTAMP"
          echo "POSTGRES_DATA_DIR: $POSTGRES_DATA_DIR"

          # If database is already marked for restoration, skip timestamp comparison
          if [ "$CHECK_DATABASE_RESTORE_NEEDED" = "true" ]; then
            echo "Database already marked for restoration from previous check."
            echo "restore_needed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Perform timestamp comparison only if database exists
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" "
            POSTGRES_DATA_DIR=\"$POSTGRES_DATA_DIR\"
            BACKUP_TIMESTAMP=\"$BACKUP_TIMESTAMP\"
            FORCE_RESTORE=\"${{ inputs.force_restore }}\"
            
            echo \"Starting remote script execution...\"
            echo \"POSTGRES_DATA_DIR=\$POSTGRES_DATA_DIR\"
            
            if [ -d \"\$POSTGRES_DATA_DIR\" ]; then
              echo \"Database directory exists, getting timestamp...\"
              sudo ls -la \"\$POSTGRES_DATA_DIR\"
              
              local_timestamp=\$(sudo stat -c %Y \"\$POSTGRES_DATA_DIR\" | sudo xargs -I{} date -d @{} '+%Y-%m-%d %H:%M:%S')
              echo \"Raw local timestamp: \$local_timestamp\"
              
              if [ -z \"\$local_timestamp\" ]; then
                echo \"ERROR: Could not get database timestamp\"
                echo \"NEEDS_RESTORE\"
                exit 0
              fi
              echo \"Local database timestamp: \$local_timestamp\"
            else
              echo \"ERROR: Database directory not found at \$POSTGRES_DATA_DIR\"
              echo \"NEEDS_RESTORE\"
              exit 0
            fi

            echo \"Converting timestamps to seconds...\"
            echo \"BACKUP_TIMESTAMP=\$BACKUP_TIMESTAMP\"
            
            backup_seconds=\$(date -d \"\$BACKUP_TIMESTAMP\" +%s)
            echo \"backup_seconds=\$backup_seconds\"
            
            local_seconds=\$(date -d \"\$local_timestamp\" +%s)
            echo \"local_seconds=\$local_seconds\"
        
            echo \"Comparing timestamps...\"
            echo \"Backup: \$BACKUP_TIMESTAMP (\$backup_seconds)\"
            echo \"Local:  \$local_timestamp (\$local_seconds)\"

            if [ \"\$backup_seconds\" -gt \"\$local_seconds\" ]; then
              echo \"Backup is newer than local database\"
              echo \"NEEDS_RESTORE\"
            elif [ \"\$FORCE_RESTORE\" = \"true\" ]; then
              echo \"Force restore requested\"
              echo \"NEEDS_RESTORE\"
            else
              echo \"Local database is up to date\"
              echo \"UP_TO_DATE\"
            fi
          ")
          
          echo "Remote script output:"
          echo "$CHECK_OUTPUT"
          
          if echo "$CHECK_OUTPUT" | grep -q "NEEDS_RESTORE"; then
            echo "Setting restore_needed=true"
            echo "restore_needed=true" >> $GITHUB_OUTPUT
          else
            echo "Setting restore_needed=false"
            echo "restore_needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Initialize PostgreSQL Data Directory
        id: init-postgres
        if: |
          steps.check-database.outputs.restore_needed == 'true' ||
          steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
          BACKUP_TYPE: ${{ inputs.backup_type }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
            POSTGRES_DB="$POSTGRES_DB" \
            POSTGRES_USER="$POSTGRES_USER" \
            POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
            POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR" \
            DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR" \
            BACKUP_TYPE="$BACKUP_TYPE" \
            'bash -s' << 'REMOTE_SCRIPT'
            set -e
            
            cleanup() {
              echo "🧹 Cleaning up Docker resources..."
              docker ps -q --filter "name=postgres" | xargs -r docker stop
              docker ps -aq --filter "name=postgres" | xargs -r docker rm
              docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm
            }
            
            # Set up cleanup trap for script exit
            trap cleanup EXIT
            
            # Initial cleanup
            echo "🧹 Initial cleanup of any existing Docker resources..."
            cleanup
            
            # Create directories
            sudo mkdir -p "$POSTGRES_DATA_DIR"
            sudo mkdir -p "$DATABASE_BACKUP_DIR"
            
            # Set up postgres user
            sudo groupadd -f -g 999 postgres || true
            if ! id postgres &>/dev/null; then
              sudo useradd -r -u 999 -g postgres postgres || true
            fi
            
            # Only initialize full PostgreSQL data directory for TAR backups
            if [ "$BACKUP_TYPE" == "tar" ]; then
              echo "🧹 Cleaning existing data directory for TAR backup..."
              sudo rm -rf "$POSTGRES_DATA_DIR"/*
              sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
              sudo chmod 0700 "$POSTGRES_DATA_DIR"
              
              echo "🐳 Starting PostgreSQL to initialize data directory for TAR backup..."
              docker run -d \
                --name postgres-init \
                -v "$POSTGRES_DATA_DIR:/var/lib/postgresql/data" \
                -e POSTGRES_DB="$POSTGRES_DB" \
                -e POSTGRES_USER="$POSTGRES_USER" \
                -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                --user 999:999 \
                postgres:14
              
              # Wait for PostgreSQL to be ready
              for i in {1..30}; do
                if docker exec postgres-init pg_isready -U "$POSTGRES_USER" &>/dev/null; then
                  echo "✅ PostgreSQL is ready"
                  break
                fi
                if [ $i -eq 30 ]; then
                  echo "❌ PostgreSQL failed to initialize"
                  exit 1
                fi
                sleep 1
              done
              
              # Stop the container - our data directory is now initialized
              docker stop postgres-init
              
              if ! sudo test -f "$POSTGRES_DATA_DIR/PG_VERSION"; then
                echo "❌ PostgreSQL initialization failed"
                exit 1
              fi
              
              echo "✅ PostgreSQL initialization completed successfully"
            elif [ "$BACKUP_TYPE" == "sql" ]; then
              echo "💾 Skipping PostgreSQL initialization for SQL backup..."
              # Ensure correct permissions for the data directory
              sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
              sudo chmod 0700 "$POSTGRES_DATA_DIR"
            fi
          REMOTE_SCRIPT

      - name: Download Latest Backup
        id: download-backup
        if: |
          steps.init-postgres.outcome == 'success' &&
          (steps.check-database.outputs.restore_needed == 'true' ||
            steps.compare-timestamp.outputs.restore_needed == 'true')
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.backup_file }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          AWS_ACCESS_KEY_ID: ${{ env.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ env.AWS_SECRET_ACCESS_KEY }}
          AWS_SESSION_TOKEN: ${{ env.AWS_SESSION_TOKEN }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
          BACKUP_TYPE: ${{ inputs.backup_type }}
        run: |
          echo "📦 Using backup file: $LATEST_BACKUP"
          echo "⏰ Backup timestamp: $BACKUP_TIMESTAMP"

          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Pass the variable explicitly to the remote script
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
            "LATEST_BACKUP='$LATEST_BACKUP' \
            S3_BACKUP_BUCKET='$S3_BACKUP_BUCKET' \
            S3_BACKUP_PREFIX='$S3_BACKUP_PREFIX' \
            AWS_ACCESS_KEY_ID='$AWS_ACCESS_KEY_ID' \
            AWS_SECRET_ACCESS_KEY='$AWS_SECRET_ACCESS_KEY' \
            AWS_SESSION_TOKEN='$AWS_SESSION_TOKEN' \
            AWS_DEFAULT_REGION='$AWS_DEFAULT_REGION' \
            DATABASE_BACKUP_DIR='$DATABASE_BACKUP_DIR' \
            BACKUP_TYPE='$BACKUP_TYPE' \
            bash -s" << 'REMOTE_SCRIPT'            

            # Debug on remote
            echo "Remote LATEST_BACKUP value: $LATEST_BACKUP"
            echo "Remote S3 path: s3://${S3_BACKUP_BUCKET}/${S3_BACKUP_PREFIX}/${LATEST_BACKUP}"
            
            set -e

            # Verify S3 bucket and object details
            echo "🔍 S3 Object Verification"
            echo "Bucket: ${S3_BACKUP_BUCKET}"
            echo "Prefix: ${S3_BACKUP_PREFIX}"
            echo "Backup File: ${LATEST_BACKUP}"
            
            # List bucket contents for debugging
            aws s3 ls "s3://${S3_BACKUP_BUCKET}/${S3_BACKUP_PREFIX}/"
            
            # Ensure backup directory exists
            mkdir -p "${DATABASE_BACKUP_DIR}"
            chmod 777 "${DATABASE_BACKUP_DIR}"
            
            # Multiple download strategies
            download_strategies=(
              "s3cp"
              "s3sync"
              "s3api"
            )

            download_success=false
            for strategy in "${download_strategies[@]}"; do
              echo "🔄 Attempting download with: $strategy"
              
              case "$strategy" in
                "s3cp")
                  if aws s3 cp "s3://${S3_BACKUP_BUCKET}/${S3_BACKUP_PREFIX}/${LATEST_BACKUP}" \
                    "${DATABASE_BACKUP_DIR}/latest_backup.${BACKUP_TYPE}.gz"; then
                    download_success=true
                    break
                  fi
                  ;;
                  
                "s3sync")
                  if aws s3 sync \
                    "s3://${S3_BACKUP_BUCKET}/${S3_BACKUP_PREFIX}/" \
                    "${DATABASE_BACKUP_DIR}" \
                    --exclude "*" \
                    --include "${LATEST_BACKUP}"; then
                    download_success=true
                    break
                  fi
                  ;;
                  
                "s3api")
                  if aws s3api get-object \
                    --bucket "${S3_BACKUP_BUCKET}" \
                    --key "${S3_BACKUP_PREFIX}/${LATEST_BACKUP}" \
                    "${DATABASE_BACKUP_DIR}/latest_backup.${BACKUP_TYPE}.gz"; then
                    download_success=true
                    break
                  fi
                  ;;
              esac
              
              echo "Strategy $strategy failed, trying next..."
            done

            if [ "$download_success" = false ]; then
              echo "❌ All download strategies failed"
              # Print the directory contents for debugging
              ls -la "${DATABASE_BACKUP_DIR}"
              # Print S3 bucket contents for debugging
              aws s3 ls "s3://${S3_BACKUP_BUCKET}/${S3_BACKUP_PREFIX}/"
              exit 1
            fi
            
            # Verify downloaded file
            backup_file="${DATABASE_BACKUP_DIR}/latest_backup.${BACKUP_TYPE}.gz"
            
            echo "🔍 Downloaded File Verification"
            ls -l "$backup_file"
            
            local_file_size=$(stat -c %s "$backup_file")
            echo "📏 Local File Size: $local_file_size bytes"
            
            if [ "$local_file_size" -eq 0 ]; then
              echo "❌ Local backup file is empty"
              exit 1
            fi
            
            echo "✅ Backup downloaded successfully"
          REMOTE_SCRIPT

      - name: Restore Database
        id: restore-database
        if: steps.download-backup.outcome == 'success'
        env:
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
          BACKUP_TYPE: ${{ inputs.backup_type }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
            "POSTGRES_HOST='$POSTGRES_HOST' \
            POSTGRES_PORT='$POSTGRES_PORT' \
            POSTGRES_DB='$POSTGRES_DB' \
            POSTGRES_USER='$POSTGRES_USER' \
            POSTGRES_PASSWORD='$POSTGRES_PASSWORD' \
            EC2_HOST='$EC2_HOST' \
            EC2_USER='$EC2_USER' \
            POSTGRES_DATA_DIR='$POSTGRES_DATA_DIR' \
            DATABASE_BACKUP_DIR='$DATABASE_BACKUP_DIR' \
            BACKUP_TYPE='$BACKUP_TYPE' \
            bash -s" << 'REMOTE_SCRIPT'   
            set -e
            set -x
            
            # Stop existing PostgreSQL container
            echo "🧹 Cleaning up Docker resources..."
            docker ps -q --filter "name=amta-postgres" | xargs -r docker stop
            docker ps -aq --filter "name=amta-postgres" | xargs -r docker rm
            docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm
            
            echo "🔄 Starting database restoration..."
            
            # Restoration based on backup type
            if [ "$BACKUP_TYPE" == "tar" ]; then
              echo "🗂️ Restoring from tar data directory backup..."
              
              TEMP_DIR=$(mktemp -d)
              trap 'sudo rm -rf "$TEMP_DIR"' EXIT
              
              sudo tar xzf "${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" -C "$TEMP_DIR" --strip-components=1
              
              sudo cp -R "$TEMP_DIR"/* "$POSTGRES_DATA_DIR"
              sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
              sudo chmod 700 "$POSTGRES_DATA_DIR"
              
            elif [ "$BACKUP_TYPE" == "sql" ]; then
              echo "📦 Restoring from SQL backup..."
              
              # Ensure data directory exists and has correct permissions
              sudo mkdir -p "$POSTGRES_DATA_DIR"
              sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
              sudo chmod 700 "$POSTGRES_DATA_DIR"
              
              # Create PostgreSQL container with persistent storage
              echo "🚀 Starting PostgreSQL container..."
              docker run -d \
                --name amta-postgres \
                -v "${POSTGRES_DATA_DIR}:/var/lib/postgresql/data" \
                -p 5432:5432 \
                -e POSTGRES_DB="$POSTGRES_DB" \
                -e POSTGRES_USER="$POSTGRES_USER" \
                -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                --user 999:999 \
                postgres:14
              
              # Wait for PostgreSQL to be ready
              echo "⏳ Waiting for PostgreSQL to start..."
              for i in {1..30}
              do
                if docker exec amta-postgres pg_isready -U "$POSTGRES_USER" &>/dev/null; then
                  echo "✅ PostgreSQL is ready"
                  
                  # Restore the SQL dump
                  echo "📥 Importing SQL backup..."
                  gunzip -c "${DATABASE_BACKUP_DIR}/latest_backup.sql.gz" | \
                    docker exec -i amta-postgres psql -U "$POSTGRES_USER" -d "$POSTGRES_DB"
                  RESTORE_STATUS=$?
                  
                  if [ $RESTORE_STATUS -eq 0 ]; then
                    echo "✅ SQL restore completed successfully"
                    
                    # Verify the restore
                    echo "🔍 Verifying database restore..."
                    docker exec amta-postgres psql -U "\$POSTGRES_USER" -d "\$POSTGRES_DB" -c \
                      "SELECT 1;" >/dev/null 2>&1
                    VERIFY_STATUS=$?
                    
                    if [ $VERIFY_STATUS -eq 0 ]; then
                      echo "Database verification successful"
                      docker stop amta-postgres
                      docker rm amta-postgres
                      break
                    else
                      echo "Database verification failed"
                      exit 1
                    fi
                  fi
                fi
                if [ $i -eq 30 ]; then
                  echo "❌ PostgreSQL initialization failed"
                  exit 1
                fi
                echo "⏳ Waiting... ($i/30)"
                sleep 1
              done
            fi
            echo "✅ Database restored successfully"
            echo "restore_status=success" >> $GITHUB_OUTPUT
          REMOTE_SCRIPT
  
  notify-status:
    if: always()
    needs: [database-restoration]
    runs-on: ubuntu-latest
    steps:
      - name: Determine Workflow Status
        run: |
          if [[ "${{ needs.database-restoration.result }}" == 'success' ]]; then
            echo "✅ Database Restoration Completed Successfully"
          else
            echo "❌ Database Restoration Failed"
            exit 1
          fi
  