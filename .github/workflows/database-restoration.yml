name: Database Restoration Workflow

on:
  workflow_dispatch:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - development
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        required: true
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: true
        default: 'amta-db'
  workflow_call:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        type: string
        required: false
        default: 'production'
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        type: string
        required: false
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: false
        default: 'amta-db'

env:
  # Use workflow input for PostgreSQL data directory
  POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
  # Use workflow input for database backups directory
  DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

permissions:
  id-token: write
  contents: read

jobs:
  database-restoration:
    runs-on: ubuntu-latest
    environment: ${{ inputs.deployment_mode }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Validate Paths
        run: |
          echo "📍 PostgreSQL Data Directory: $POSTGRES_DATA_DIR"
          echo "📦 Database Backup Directory: $DATABASE_BACKUP_DIR"
          
          # Validate paths are not empty
          if [ -z "$POSTGRES_DATA_DIR" ]; then
            echo "❌ POSTGRES_DATA_DIR must be set"
            exit 1
          fi
          
          if [ -z "$DATABASE_BACKUP_DIR" ]; then
            echo "❌ DATABASE_BACKUP_DIR must be set"
            exit 1
          fi

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          if ! secrets=$(aws secretsmanager get-secret-value --secret-id amta-${{ inputs.deployment_mode }}-secrets --query SecretString --output text); then
            echo "Failed to retrieve secrets from AWS Secrets Manager"
            exit 1
          fi
          
          # Parse database connection details
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          if [ -z "$database_url" ]; then
            echo "DATABASE_URL not found in secrets"
            exit 1
          fi
          
          # Extract database connection parameters
          postgres_host=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@([^:/]+).*|\1|')
          postgres_db=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?[0-9]*/([^?]+).*|\1|')
          postgres_user=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://([^:]+):.*|\1|')
          postgres_password=$(echo "$secrets" | jq -r '.POSTGRES_PASSWORD')
          postgres_port=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?([0-9]+)?/.*|\1|')
          if [ -z "$postgres_port" ]; then
              postgres_port="5432"
          fi
          # Validate extracted values
          for var in postgres_host postgres_db postgres_user postgres_password; do
            if [ -z "${!var}" ]; then
              echo "Failed to extract $var"
              exit 1
            fi
          done

          # Set outputs
          {
            echo "POSTGRES_HOST=$postgres_host"
            echo "POSTGRES_PORT=$postgres_port"
            echo "POSTGRES_DB=$postgres_db"
            echo "POSTGRES_USER=$postgres_user"
            echo "POSTGRES_PASSWORD=$postgres_password"
          } >> "$GITHUB_OUTPUT"

      - name: Prepare SSH Key
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
        run: |
          if [ -z "$EC2_SSH_KEY" ]; then
            echo "EC2 SSH key is not set"
            exit 1
          fi
          
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key

      - name: Check Database Initialization Status
        id: check-database
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}

        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e

            # Function to check PostgreSQL initialization status
            check_postgres_initialization() {
              local data_dir="$POSTGRES_DATA_DIR"
              local status="UNINITIALIZED"
              
              echo "🔍 Checking PostgreSQL data directory: $data_dir"
              
              # Debug: Show directory contents and permissions
              echo "Directory listing:"
              sudo ls -la "$data_dir" || echo "Directory does not exist or not accessible"
              
              # Debug: Try to find PG_VERSION
              echo "Searching for PG_VERSION:"
              sudo find "$data_dir" -name PG_VERSION 2>/dev/null || echo "PG_VERSION not found"
              
              # Step 1: Check if directory exists and has basic structure
              if sudo test -d "$data_dir" && sudo test -f "$data_dir/PG_VERSION"; then
                echo "Found PG_VERSION file"
                
                # Check for essential PostgreSQL directories
                local required_dirs=("base" "global" "pg_wal" "pg_xact")
                local missing_dirs=0
                
                for dir in "${required_dirs[@]}"; do
                  echo "Checking directory: $dir"
                  if ! sudo test -d "$data_dir/$dir"; then
                    echo "Missing directory: $dir"
                    missing_dirs=$((missing_dirs + 1))
                  else
                    echo "Found directory: $dir"
                  fi
                done
                
                if [ $missing_dirs -eq 0 ]; then
                  echo "All required directories present"
                  status="INITIALIZED"
                else
                  echo "Missing $missing_dirs required directories"
                  status="CORRUPTED"
                fi
              fi
              
              # Step 2: If initialized, verify with temporary container
              if [ "$status" = "INITIALIZED" ]; then
                echo "Verifying PostgreSQL data with temporary container..."
                
                # Clean up any existing containers
                docker ps -q --filter "name=pg-verify" | xargs -r docker stop
                docker ps -aq --filter "name=pg-verify" | xargs -r docker rm
                
                # Debug: Show data directory permissions before container start
                echo "Data directory permissions:"
                sudo ls -la "$data_dir"
                
                # Ensure proper permissions for Docker
                echo "Setting proper permissions for Docker..."
                sudo chown -R 999:999 "$data_dir"
                sudo chmod 0700 "$data_dir"
                
                if ! timeout 30s docker run --rm \
                  --name pg-verify \
                  -v "$data_dir:/var/lib/postgresql/data" \
                  -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                  -e POSTGRES_USER="$POSTGRES_USER" \
                  -e POSTGRES_DB="$POSTGRES_DB" \
                  --user 999:999 \
                  postgres:14 \
                  postgres -D /var/lib/postgresql/data -c 'log_statement=none' &> /dev/null; then
                  echo "Container verification failed"
                  status="CORRUPTED"
                else
                  echo "Container verification successful"
                fi
              fi
              
              echo "Final status: $status"
              echo "$status"
            }
            
            # Main execution
            echo "Starting database initialization check..."
            status=$(check_postgres_initialization)
            echo "Check completed with status: $status"
            
            # Store status in a variable instead of exiting
            case "$status" in
              "UNINITIALIZED")
                echo "Database is not initialized"
                db_status=2
                ;;
              "CORRUPTED")
                echo "Database is corrupted or incomplete"
                db_status=3
                ;;
              "INITIALIZED")
                echo "Database is properly initialized"
                db_status=0
                ;;
              *)
                echo "Unknown database status"
                db_status=1
                ;;
            esac
            
            # Return the status code without exiting
            echo "DB_STATUS=$db_status"
          REMOTE_SCRIPT
          )
          
          # Extract the status code from the output
          if echo "$CHECK_OUTPUT" | grep -q "DB_STATUS="; then
            db_status=$(echo "$CHECK_OUTPUT" | grep "DB_STATUS=" | cut -d'=' -f2)
          else
            db_status=1
          fi
          
          # Print output for logging
          echo "Check output: $CHECK_OUTPUT"
          
          # Set GitHub outputs based on status
          case $db_status in
            0)
              echo "status=initialized" >> $GITHUB_OUTPUT
              echo "restore_needed=false" >> $GITHUB_OUTPUT
              ;;
            2)
              echo "status=uninitialized" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
            3)
              echo "status=corrupted" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "status=unknown" >> $GITHUB_OUTPUT
              echo "restore_needed=true" >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Discover Latest S3 Backup
        id: discover-backup
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
        run: |
          # List and sort backups by date, get the latest
          latest_backup=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/" \
            | grep '\.tar\.gz$' \
            | sort -k1,2 \
            | tail -n 1 \
            | awk '{print $4}')
          
          if [ -z "$latest_backup" ]; then
            echo "❌ No backup files found in S3"
            exit 1
          fi

          # Get backup timestamp
          backup_timestamp=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$latest_backup" | awk '{print $1 " " $2}')
          
          echo "Latest backup: $latest_backup"
          echo "Backup timestamp: $backup_timestamp"
          echo "latest_backup=$latest_backup" >> $GITHUB_OUTPUT
          echo "backup_timestamp=$backup_timestamp" >> $GITHUB_OUTPUT

      - name: Compare Backup Timestamp
        id: compare-timestamp
        if: steps.check-database.outputs.status == 'initialized'
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          BACKUP_TIMESTAMP: ${{ steps.discover-backup.outputs.backup_timestamp }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e
            
            # Get local database timestamp from pg_controldata
            local_timestamp=$(sudo -u postgres pg_controldata "$POSTGRES_DATA_DIR" | grep "Database cluster state:" | awk '{print $4 " " $5}')
            
            # Convert timestamps to seconds since epoch
            backup_seconds=$(date -d "$BACKUP_TIMESTAMP" +%s)
            local_seconds=$(date -d "$local_timestamp" +%s)
            
            # Store comparison result in variable instead of exiting
            if [ "$backup_seconds" -gt "$local_seconds" ] || [ "${{ inputs.force_restore }}" = "true" ]; then
              echo "NEEDS_RESTORE"
            else
              echo "UP_TO_DATE"
            fi
          REMOTE_SCRIPT
          )
          
          # Set GitHub output based on comparison result
          if echo "$CHECK_OUTPUT" | grep -q "NEEDS_RESTORE"; then
            echo "restore_needed=true" >> $GITHUB_OUTPUT
          else
            echo "restore_needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Initialize PostgreSQL Data Directory
        id: init-postgres
        if: |
          steps.check-database.outputs.restore_needed == 'true' ||
          steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" \
            POSTGRES_DB="$POSTGRES_DB" \
            POSTGRES_USER="$POSTGRES_USER" \
            POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
            POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR" \
            DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR" \
            'bash -s' << 'REMOTE_SCRIPT'
            set -e
            
            cleanup() {
              echo "🧹 Cleaning up Docker resources..."
              docker ps -q --filter "name=postgres" | xargs -r docker stop
              docker ps -aq --filter "name=postgres" | xargs -r docker rm
              docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm
            }
          
            # Set up cleanup trap for script exit
            trap cleanup EXIT
            
            # Initial cleanup
            echo "🧹 Initial cleanup of any existing Docker resources..."
            cleanup
            
            # Create directories
            sudo mkdir -p "$POSTGRES_DATA_DIR"
            sudo mkdir -p "$DATABASE_BACKUP_DIR"
            
            # Set up postgres user
            sudo groupadd -f -g 999 postgres || true
            if ! id postgres &>/dev/null; then
              sudo useradd -r -u 999 -g postgres postgres || true
            fi
            
            # Prepare data directory
            sudo rm -rf "$POSTGRES_DATA_DIR"/*
            sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
            sudo chmod 0700 "$POSTGRES_DATA_DIR"
            
            echo "🐳 Starting PostgreSQL to initialize data directory..."
            # The postgres image will automatically initialize the data directory
            # and create the database if it's empty
            docker run -d \
              --name postgres-init \
              -v "$POSTGRES_DATA_DIR:/var/lib/postgresql/data" \
              -e POSTGRES_DB="$POSTGRES_DB" \
              -e POSTGRES_USER="$POSTGRES_USER" \
              -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
              --user 999:999 \
              postgres:14
            
            # Wait for PostgreSQL to be ready
            for i in {1..30}; do
              if docker exec postgres-init pg_isready -U "$POSTGRES_USER" &>/dev/null; then
                echo "✅ PostgreSQL is ready"
                break
              fi
              if [ $i -eq 30 ]; then
                echo "❌ PostgreSQL failed to initialize"
                exit 1
              fi
              sleep 1
            done
            
            # Stop the container - our data directory is now initialized
            docker stop postgres-init
            
            if ! sudo test -f "$POSTGRES_DATA_DIR/PG_VERSION"; then
              echo "❌ PostgreSQL initialization failed"
              exit 1
            fi
            
            echo "✅ PostgreSQL initialization completed successfully"
          REMOTE_SCRIPT

      - name: Download Latest Backup
        id: download-backup
        if: |
          steps.init-postgres.outcome == 'success' &&
          (steps.check-database.outputs.restore_needed == 'true' ||
           steps.compare-timestamp.outputs.restore_needed == 'true')
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          AWS_ACCESS_KEY_ID: ${{ env.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ env.AWS_SECRET_ACCESS_KEY }}
          AWS_SESSION_TOKEN: ${{ env.AWS_SESSION_TOKEN }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e
            
            # Export AWS credentials
            export AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID"
            export AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY"
            export AWS_SESSION_TOKEN="$AWS_SESSION_TOKEN"
            export AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION"
            
            echo "⬇️ Downloading backup from S3..."
            aws s3 cp "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$LATEST_BACKUP" \
              "$DATABASE_BACKUP_DIR/latest_backup.tar.gz"
            
            if [ ! -f "${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" ]; then
              echo "❌ Backup download failed"
              exit 1
            fi
            
            # Verify backup integrity
            if ! tar tzf "${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" > /dev/null 2>&1; then
              echo "❌ Invalid backup archive"
              exit 1
            fi
            
            # Check backup contents
            backup_contents=$(tar tzf "${DATABASE_BACKUP_DIR}/latest_backup.tar.gz")
            if ! echo "$backup_contents" | grep -q "PG_VERSION"; then
              echo "❌ Invalid PostgreSQL backup"
              exit 1
            fi
            
            if ! echo "$backup_contents" | grep -q "base/"; then
              echo "❌ Missing essential PostgreSQL directories"
              exit 1
            fi
            
            echo "✅ Backup downloaded and verified successfully"
          REMOTE_SCRIPT

      - name: Restore Database
        id: restore-database
        if: steps.download-backup.outcome == 'success'
        env:
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e
            
            cleanup() {
              echo "🧹 Cleaning up Docker resources..."
              docker ps -q --filter "name=postgres-restore" | xargs -r docker stop
              docker ps -aq --filter "name=postgres-restore" | xargs -r docker rm
              docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm
            }
            
            trap cleanup EXIT
            
            # Stop existing PostgreSQL container
            if docker ps -q --filter "name=amta-postgres" >/dev/null; then
              docker exec amta-postgres psql -U "$POSTGRES_USER" -d postgres -c \
                "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB' AND pid <> pg_backend_pid();" || true
              sleep 2
              docker stop amta-postgres || true
            fi
            
            cleanup
            
            echo "🔄 Starting database restoration..."
            
            # Extract backup
            TEMP_DIR=$(mktemp -d)
            trap 'sudo rm -rf "$TEMP_DIR"' EXIT
            
            sudo tar xzf "${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" -C "$TEMP_DIR" --strip-components=1
            
            # Determine backup type and restore
            if [ -f "$TEMP_DIR/PG_VERSION" ]; then
              echo "🗂️ Restoring from data directory backup..."
              
              sudo cp -R "$TEMP_DIR"/* "$POSTGRES_DATA_DIR"
              sudo chown -R 999:999 "$POSTGRES_DATA_DIR"
              sudo chmod 700 "$POSTGRES_DATA_DIR"
              
              # Verify restoration
              echo "🔍 Verifying restored data..."
              docker run -d \
                --name postgres-verify \
                -v "$POSTGRES_DATA_DIR:/var/lib/postgresql/data" \
                -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                -p 5433:5432 \
                --user 999:999 \
                postgres:14
              
              for i in {1..30}; do
                if docker exec postgres-verify pg_isready -U "$POSTGRES_USER" -p 5432 &>/dev/null; then
                  if docker exec postgres-verify psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c \
                    "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" | grep -q '[1-9]'; then
                    echo "✅ Database verification successful"
                    docker stop postgres-verify
                    docker rm postgres-verify
                    break
                  fi
                fi
                if [ $i -eq 30 ]; then
                  echo "❌ Database verification failed"
                  exit 1
                fi
                sleep 2
              done
              
            else
              echo "❌ Invalid backup format"
              exit 1
            fi
            
            echo "✅ Database restored successfully"
            echo "restore_status=success" >> $GITHUB_OUTPUT
          REMOTE_SCRIPT

  notify-status:
    if: always()
    needs: [database-restoration]
    runs-on: ubuntu-latest
    steps:
      - name: Determine Workflow Status
        run: |
          if [[ "${{ needs.database-restoration.result }}" == 'success' ]]; then
            echo "✅ Database Restoration Completed Successfully"
          else
            echo "❌ Database Restoration Failed"
            exit 1
          fi
