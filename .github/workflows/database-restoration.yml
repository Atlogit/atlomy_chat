name: Database Restoration Workflow

on:
  workflow_dispatch:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - development
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        required: true
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: true
        default: 'amta-db'
  workflow_call:
    inputs:
      deployment_mode:
        description: 'Deployment mode (development/production)'
        type: string
        required: false
        default: 'production'
      postgres_data_dir:
        description: 'PostgreSQL Data Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/postgresql/data'
      database_backup_dir:
        description: 'Database Backup Directory'
        type: string
        required: false
        default: '/home/ec2-user/amta/database_backups'
      force_restore:
        description: 'Force database restoration'
        type: boolean
        default: false
      s3_backup_bucket:
        description: 'S3 Bucket for Database Backups'
        type: string
        required: false
        default: 'amta-app'
      s3_backup_prefix:
        description: 'S3 Prefix for Database Backups'
        type: string
        required: false
        default: 'amta-db'

env:
  # Use workflow input for PostgreSQL data directory
  POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
  # Use workflow input for database backups directory
  DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

permissions:
  id-token: write
  contents: read

jobs:
  database-restoration:
    runs-on: ubuntu-latest
    environment: ${{ inputs.deployment_mode }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Validate Paths
        run: |
          echo "üìç PostgreSQL Data Directory: $POSTGRES_DATA_DIR"
          echo "üì¶ Database Backup Directory: $DATABASE_BACKUP_DIR"
          
          # Validate paths are not empty
          if [ -z "$POSTGRES_DATA_DIR" ]; then
            echo "‚ùå POSTGRES_DATA_DIR must be set"
            exit 1
          fi
          
          if [ -z "$DATABASE_BACKUP_DIR" ]; then
            echo "‚ùå DATABASE_BACKUP_DIR must be set"
            exit 1
          fi

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          if ! secrets=$(aws secretsmanager get-secret-value --secret-id amta-${{ inputs.deployment_mode }}-secrets --query SecretString --output text); then
            echo "Failed to retrieve secrets from AWS Secrets Manager"
            exit 1
          fi
          
          # Parse database connection details
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          if [ -z "$database_url" ]; then
            echo "DATABASE_URL not found in secrets"
            exit 1
          fi
          
          # Extract database connection parameters
          postgres_host=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@([^:/]+).*|\1|')
          postgres_port=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?([0-9]+)?.*|\1|' || echo "5432")
          postgres_db=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://[^:]+:[^@]+@[^:/]+:?[0-9]*/([^?]+).*|\1|')
          postgres_user=$(echo "$database_url" | sed -E 's|postgresql\+asyncpg://([^:]+):.*|\1|')
          postgres_password=$(echo "$secrets" | jq -r '.POSTGRES_PASSWORD')
          
          # Validate extracted values
          for var in postgres_host postgres_db postgres_user postgres_password; do
            if [ -z "${!var}" ]; then
              echo "Failed to extract $var"
              exit 1
            fi
          done

          # Set outputs
          {
            echo "POSTGRES_HOST=$postgres_host"
            echo "POSTGRES_PORT=$postgres_port"
            echo "POSTGRES_DB=$postgres_db"
            echo "POSTGRES_USER=$postgres_user"
            echo "POSTGRES_PASSWORD=$postgres_password"
          } >> "$GITHUB_OUTPUT"

      - name: Prepare SSH Key
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
        run: |
          if [ -z "$EC2_SSH_KEY" ]; then
            echo "EC2 SSH key is not set"
            exit 1
          fi
          
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key

      - name: Check Database Initialization Status
        id: check-database
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e

            export POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR"

            echo "Checking PostgreSQL data directory: $POSTGRES_DATA_DIR"
            # Check if PostgreSQL data directory exists and contains PG_VERSION
            if ! sudo test -f "$POSTGRES_DATA_DIR/PG_VERSION"; then
              echo "Database not initialized. Restoration required."
              exit 2
            else
              echo "Database directory exists."
              exit 0
            fi
          REMOTE_SCRIPT
            )
            # Capture the exit code
            SSH_EXIT_CODE=$?
            
            # Print the output for logging
            echo "$CHECK_OUTPUT"
            
            # Set the GitHub output based on exit code
            if [ $SSH_EXIT_CODE -eq 2 ]; then
              echo "restore_needed=true" >> $GITHUB_OUTPUT
            else
              echo "restore_needed=false" >> $GITHUB_OUTPUT
            fi
            
      - name: Discover Latest S3 Backup
        id: discover-backup
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
        run: |
          # List and sort backups by date, get the latest
          latest_backup=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/" \
            | grep '\.tar\.gz$' \
            | sort -k1,2 \
            | tail -n 1 \
            | awk '{print $4}')
          
          if [ -z "$latest_backup" ]; then
            echo "‚ùå No backup files found in S3"
            exit 1
          fi

          # Get backup timestamp
          backup_timestamp=$(aws s3 ls "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$latest_backup" | awk '{print $1 " " $2}')
          
          echo "Latest backup: $latest_backup"
          echo "Backup timestamp: $backup_timestamp"
          echo "latest_backup=$latest_backup" >> $GITHUB_OUTPUT
          echo "backup_timestamp=$backup_timestamp" >> $GITHUB_OUTPUT

      - name: Compare Backup Timestamp
        id: compare-timestamp
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          BACKUP_TIMESTAMP: ${{ steps.discover-backup.outputs.backup_timestamp }}
          CHECK_DATABASE_RESTORE_NEEDED: ${{ steps.check-database.outputs.restore_needed }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}

        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # If database is already marked for restoration, skip timestamp comparison
          if [ "$CHECK_DATABASE_RESTORE_NEEDED" = "true" ]; then
            echo "Database already marked for restoration from previous check."
            echo "restore_needed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Perform timestamp comparison only if database exists
          CHECK_OUTPUT=$(ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
            
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"

            # Get local database timestamp
            # Check if PG_VERSION exists
            if ! sudo test -f "$DATABASE_BACKUP_DIR/PG_VERSION"; then
              echo "No existing database found. Restoration needed."
              exit 2
            fi        

            # Convert timestamps to seconds since epoch
            backup_seconds=\$(date -d "$BACKUP_TIMESTAMP" +%s)
            local_seconds=\$(date -d "\$local_timestamp" +%s)
      
            # Compare timestamps
            if [ "\$backup_seconds" -gt "\$local_seconds" ] || [ "${{ inputs.force_restore }}" = "true" ]; then
              echo "Backup is newer than local database or force restore requested."
              exit 2
            else
              echo "Local database is up to date."
              exit 0
            fi
          REMOTE_SCRIPT
            )
            # Capture the exit code
            SSH_EXIT_CODE=$?
            
            # Print the output for logging
            echo "$CHECK_OUTPUT"
            
            # Set the GitHub output based on exit code
            if [ $SSH_EXIT_CODE -eq 2 ]; then
              echo "restore_needed=true" >> $GITHUB_OUTPUT
            else
              echo "restore_needed=false" >> $GITHUB_OUTPUT
            fi
            
      - name: Initialize PostgreSQL Data Directory
        id: init-postgres
        if: steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
      
        run: |

          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Verify environment variables
          for var in POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD EC2_HOST EC2_USER; do
            if [ -z "${!var}" ]; then
              echo "Error: $var is not set"
              exit 1
            fi
          done

          # Execute remote initialization
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
            
            cleanup() {
              echo "üßπ Cleaning up Docker resources..."
              # Stop the compose service if running
              if [ -f "docker-compose.yml" ]; then
                docker compose down || true
              fi
              # Stop any running postgres containers
              docker ps -q --filter "name=postgres" --filter "ancestor=postgres" | xargs -r docker stop || true
              # Remove any stopped postgres containers
              docker ps -aq --filter "name=postgres" --filter "ancestor=postgres" | xargs -r docker rm || true
              # Remove any dangling volumes
              docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm || true
            }
          
            # Set up cleanup trap for script exit
            trap cleanup EXIT
            
            # Initial cleanup
            echo "üßπ Initial cleanup of any existing Docker resources..."
            cleanup

            # Export variables for use in the script
            export EC2_USER="$EC2_USER"
            export POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR"
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"
      
            # Define directories using the current user
            echo "üîÑ Starting PostgreSQL initialization in $POSTGRES_DATA_DIR..."
            
            # Create postgres user and group with specific UID/GID (matching Docker postgres image)
            sudo groupadd -f -g 999 postgres || true
            if ! id postgres &>/dev/null; then
              sudo useradd -r -u 999 -g postgres postgres || true
            fi

            # Create parent directories with EC2 user ownership
            sudo mkdir -p "\$(dirname \$POSTGRES_DATA_DIR)"
            sudo mkdir -p "\$(dirname \$DATABASE_BACKUP_DIR)"
            sudo chown -R "\$EC2_USER:\$EC2_USER" "\$(dirname \$POSTGRES_DATA_DIR)"
            sudo chown -R "\$EC2_USER:\$EC2_USER" "\$(dirname \$DATABASE_BACKUP_DIR)"
            
            # Create backup directory with EC2 user ownership
            mkdir -p "\$DATABASE_BACKUP_DIR"
            chmod 755 "\$DATABASE_BACKUP_DIR"
            
            # Clean and prepare PostgreSQL data directory
            sudo rm -rf "\$POSTGRES_DATA_DIR"
            sudo mkdir -p "\$POSTGRES_DATA_DIR"
            sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
            sudo chmod 0777 "\$POSTGRES_DATA_DIR"  # Temporarily more permissive

            # Backup existing data if present (as EC2 user)
            if [ -d "\$POSTGRES_DATA_DIR" ] && [ "\$(sudo ls -A \$POSTGRES_DATA_DIR)" ]; then
              timestamp=\$(date +%Y%m%d_%H%M%S)
              echo "üì¶ Backing up existing data..."
              sudo tar czf "\${DATABASE_BACKUP_DIR}/pg_data_backup_\${timestamp}.tar.gz" \
                -C "\$(dirname \$POSTGRES_DATA_DIR)" "\$(basename \$POSTGRES_DATA_DIR)"
              sudo rm -rf "\$POSTGRES_DATA_DIR"/*
            fi

            # Verify Docker is available
            if ! command -v docker &> /dev/null; then
              echo "‚ùå Docker is not installed"
              exit 1
            fi

            # Run Docker initialization
            echo "üê≥ Initializing PostgreSQL using Docker..."
            # Set timeout for Docker command
            timeout 300 sudo docker run --rm \
              --name postgres-restore \
              -v "\$POSTGRES_DATA_DIR:\$POSTGRES_DATA_DIR" \
              -e POSTGRES_DB="$POSTGRES_DB" \
              -e POSTGRES_USER="$POSTGRES_USER" \
              -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
              --user 999:999 \
              postgres:14 \
              initdb -D "\$POSTGRES_DATA_DIR" || {
                echo "‚ùå Docker initialization timed out or failed"
                exit 1;
              }
      
            # Ensure final permissions are correct
            sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
            sudo chmod 0700 "\$POSTGRES_DATA_DIR"
            
            echo "‚úÖ PostgreSQL initialization completed successfully"
          REMOTE_SCRIPT

      - name: Download Latest Backup
        id: download-backup
        if: steps.compare-timestamp.outputs.restore_needed == 'true'
        env:
          S3_BACKUP_BUCKET: ${{ inputs.s3_backup_bucket }}
          S3_BACKUP_PREFIX: ${{ inputs.s3_backup_prefix }}
          LATEST_BACKUP: ${{ steps.discover-backup.outputs.latest_backup }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          AWS_ACCESS_KEY_ID: ${{ env.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ env.AWS_SECRET_ACCESS_KEY }}
          AWS_SESSION_TOKEN: ${{ env.AWS_SESSION_TOKEN }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
      
        run: |
          echo "üîç Downloading backup: $LATEST_BACKUP"
          
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          # Transfer and verify backup
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << REMOTE_SCRIPT
            set -e
            
            # Export AWS credentials in the remote session
            export AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID"
            export AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY"
            export AWS_SESSION_TOKEN="$AWS_SESSION_TOKEN"
            export AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION"
            
            # Export variables for use in the script
            export EC2_USER="$EC2_USER"
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"

            # Ensure backup directory exists
            sudo mkdir -p "\$DATABASE_BACKUP_DIR"
            sudo chown -R "\$EC2_USER:\$EC2_USER" "\$DATABASE_BACKUP_DIR"
            
            echo "‚¨áÔ∏è Downloading backup from S3..."
            aws s3 cp "s3://$S3_BACKUP_BUCKET/$S3_BACKUP_PREFIX/$LATEST_BACKUP" \
              "$DATABASE_BACKUP_DIR/latest_backup.tar.gz"
            
            if [ ! -f "\${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" ]; then
              echo "‚ùå Backup download failed"
              exit 1
            fi
            
            # Verify backup integrity and structure
            if ! tar tzf "\${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" > /dev/null 2>&1; then
              echo "‚ùå Backup file is not a valid tar archive"
              exit 1
            fi

            backup_contents=$(tar tzf \${DATABASE_BACKUP_DIR}/latest_backup.tar.gz | cut -d'/' -f2-)
            
            # Check if the backup contains a PostgreSQL data directory
            if ! echo "$backup_contents" | grep -q "PG_VERSION"; then
              echo "‚ùå Backup does not appear to be a valid PostgreSQL data directory"
              exit 1
            fi
            
            # Additional checks for PostgreSQL directory structure
            if ! echo "$backup_contents" | grep -q "base/"; then
              echo "‚ùå Backup is missing essential PostgreSQL data subdirectories"
              exit 1
            fi
            
            echo "‚úÖ Backup downloaded and verified successfully"
          REMOTE_SCRIPT

          # Verify initialization

      - name: Restore Database
        id: restore-database
        if: steps.download-backup.outcome == 'success'
        env:
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ steps.get-database-secrets.outputs.POSTGRES_PASSWORD }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          POSTGRES_DATA_DIR: ${{ inputs.postgres_data_dir }}
          DATABASE_BACKUP_DIR: ${{ inputs.database_backup_dir }}
      
        run: |
          ssh_opts="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"
          
          ssh $ssh_opts -i ~/.ssh/ec2_key "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
            set -e

            cleanup() {
              echo "üßπ Cleaning up temporary Docker resources..."
              # Stop and remove the postgres container if it exists
              docker ps -q --filter "name=postgres-restore" | xargs -r docker stop || true
              docker ps -aq --filter "name=postgres-restore" | xargs -r docker rm || true
              # Clean up any dangling volumes
              docker volume ls -q --filter "dangling=true" | xargs -r docker volume rm || true
            }
          
            # Set up cleanup trap
            trap cleanup EXIT INT TERM
          
            # Stop the running PostgreSQL container (will be handled by main deployment workflow)
            echo "üõë Stopping running PostgreSQL container..."
            if docker ps -q --filter "name=amta-postgres" >/dev/null; then
              # Gracefully terminate connections before stopping
              docker exec amta-postgres psql -U "$POSTGRES_USER" -d postgres -c \
                "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB' AND pid <> pg_backend_pid();" || true
              
              # Give connections a moment to terminate
              sleep 2
              
              # Stop the container
              docker stop amta-postgres || true
            fi

            # Initial cleanup
            cleanup

            # Export variables for use in the script
            export EC2_USER="$EC2_USER"
            export POSTGRES_DATA_DIR="$POSTGRES_DATA_DIR"
            export DATABASE_BACKUP_DIR="$DATABASE_BACKUP_DIR"
            export PGPASSWORD="$POSTGRES_PASSWORD"
            
            echo "üîÑ Starting database restoration process..."
            
            # Extract backup
            echo "üì¶ Extracting backup..."
            # Create temporary directory for extraction
            TEMP_DIR=\$(mktemp -d)
            trap 'sudo rm -rf "\$TEMP_DIR"' EXIT
            
            sudo tar xzf "\${DATABASE_BACKUP_DIR}/latest_backup.tar.gz" -C "\$TEMP_DIR" --strip-components=1
            
            # Determine backup type
            DUMP_FILE=\$(sudo find "\$TEMP_DIR" -name "*.dump" -type f)
            PG_VERSION_FILE=\$(sudo find "\$TEMP_DIR" -name "PG_VERSION" -type f)
            
            if [ -n "\$PG_VERSION_FILE" ]; then
              echo "üóÇÔ∏è Full PostgreSQL data directory backup detected"
              
              # Copy entire data directory
              sudo cp -R "\$TEMP_DIR"/* "\$POSTGRES_DATA_DIR"
              # Ensure correct ownership and permissions
              sudo chown -R 999:999 "\$POSTGRES_DATA_DIR"
              sudo chmod 700 "\$POSTGRES_DATA_DIR"     
              
              # Verify using temporary container
              echo "üê≥ Starting temporary PostgreSQL to verify data..."
              docker run -d \
                --name postgres-temp-restore \
                -v "\$POSTGRES_DATA_DIR:\$POSTGRES_DATA_DIR" \
                -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                -p 5433:5432 \
                --user 999:999 \
                postgres:14
                
              #Wait for PostgreSQL to start
              echo "‚è≥ Verifying restored data..."
              for i in {1..30}; do
                if docker exec postgres-temp-restore pg_isready -U "$POSTGRES_USER" -p 5432 &>/dev/null; then
                  if docker exec postgres-temp-restore psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" | grep -q '[1-9]'; then
                    echo "‚úÖ Database verification successful"
                    docker stop postgres-temp-restore
                    docker rm postgres-temp-restore
                    break
                  fi
                fi
                if [ \$i -eq 30 ]; then
                  echo "‚ùå Database verification failed"
                  exit 1
                fi
                sleep 2
              done

            elif [ -n "\$DUMP_FILE" ]; then
              echo "üìÑ Database dump file detected"
              
              # Start temporary PostgreSQL for restoration
              echo "üê≥ Starting temporary PostgreSQL for restoration..."
              docker run -d \
                --name postgres-temp-restore \
                -v "\$POSTGRES_DATA_DIR:\$POSTGRES_DATA_DIR" \
                -e POSTGRES_DB="$POSTGRES_DB" \
                -e POSTGRES_USER="$POSTGRES_USER" \
                -e POSTGRES_PASSWORD="$POSTGRES_PASSWORD" \
                -p 5433:5432 \
                postgres:14
              
              # Wait for PostgreSQL to start
              echo "‚è≥ Waiting for PostgreSQL to start..."
              for i in {1..30}; do
                if docker exec postgres-temp-restore pg_isready -U "$POSTGRES_USER" -p 5432 &>/dev/null; then
                  echo "‚úÖ PostgreSQL ready for restoration"
                  break
                fi
                if [ \$i -eq 30 ]; then
                  echo "‚ùå PostgreSQL failed to start"
                  exit 1
                fi
                sleep 2
              done

              # Restore from dump
              echo "üì• Restoring from dump file..."
              docker exec -i postgres-temp-restore pg_restore \
                --clean --if-exists --no-owner --no-privileges \
                -U "$POSTGRES_USER" -d "$POSTGRES_DB" < "\$DUMP_FILE"
              
              # Verify restoration
              if ! docker exec postgres-temp-restore psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" | grep -q '[1-9]'; then
                echo "‚ùå Database restoration from dump failed or resulted in empty database"
                exit 1
              fi
              
              echo "‚úÖ Database restored successfully"
              
              # Stop and remove the temporary container
              docker stop postgres-temp-restore
              docker rm postgres-temp-restore
              
            else
              echo "‚ùå No valid backup found: neither PostgreSQL data directory nor dump file detected"
              exit 1
            fi
            
            echo "‚úÖ Database restored successfully"
            echo "restore_status=success" >> $GITHUB_OUTPUT
          REMOTE_SCRIPT

  notify-status:
    if: always()
    needs: [database-restoration]
    runs-on: ubuntu-latest
    steps:
      - name: Determine Workflow Status
        run: |
          if [[ "${{ needs.database-restoration.result }}" == 'success' ]]; then
            echo "‚úÖ Database Restoration Completed Successfully"
          else
            echo "‚ùå Database Restoration Failed"
            exit 1
          fi
