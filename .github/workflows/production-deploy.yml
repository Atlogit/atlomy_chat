name: Production Deployment Workflow

on:
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Force deployment without Docker Build workflow'
        required: false
        type: boolean
        default: false
  workflow_run:
    workflows: ["Docker Build and Registry Push"]
    types:
      - completed
    branches:
      - production

# Comprehensive permissions for GitHub Actions workflow
permissions:
  contents: read
  packages: read
  id-token: write

env:
  BACKEND_IMAGE_NAME: ${{ github.repository }}-backend
  FRONTEND_IMAGE_NAME: ${{ github.repository }}-frontend

jobs:
  validate-workflow-trigger:
    runs-on: ubuntu-latest
    steps:
      - name: Validate Workflow Trigger
        run: |
          echo "üîç Workflow Trigger Analysis:"
          echo "Event Name: ${{ github.event_name }}"
          echo "Workflow Run Conclusion: ${{ github.event.workflow_run.conclusion }}"
          echo "Workflow Run Name: ${{ github.event.workflow_run.name }}"
          echo "Workflow Run Branch: ${{ github.event.workflow_run.head_branch }}"

  database-restoration:
    needs: validate-workflow-trigger
    uses: ./.github/workflows/database-restoration.yml
    secrets: inherit
    with:
      deployment_mode: production

  database-validation:
    needs: database-restoration
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          secrets=$(aws secretsmanager get-secret-value --secret-id amta-production-secrets --query SecretString --output text)
          
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          postgres_host=$(echo "$secrets" | jq -r '.POSTGRES_HOST')
          postgres_port=$(echo "$secrets" | jq -r '.POSTGRES_PORT')
          postgres_db=$(echo "$secrets" | jq -r '.POSTGRES_DB')
          postgres_user=$(echo "$secrets" | jq -r '.POSTGRES_USER')
          
          echo "DATABASE_URL=$database_url" >> $GITHUB_OUTPUT
          echo "POSTGRES_HOST=$postgres_host" >> $GITHUB_OUTPUT
          echo "POSTGRES_PORT=$postgres_port" >> $GITHUB_OUTPUT
          echo "POSTGRES_DB=$postgres_db" >> $GITHUB_OUTPUT
          echo "POSTGRES_USER=$postgres_user" >> $GITHUB_OUTPUT

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install sqlalchemy[asyncio] asyncpg psycopg2-binary greenlet

      - name: Validate Restored Database
        env:
          DATABASE_URL: ${{ steps.get-database-secrets.outputs.DATABASE_URL }}
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
        run: |
          python3 - << EOF
          import os
          import asyncio
          import time
          from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
          from sqlalchemy.orm import sessionmaker
          from sqlalchemy import text
          import asyncpg

          async def validate_database_schema(engine):
              validation_results = {
                  'tables': {},
                  'indexes': {},
                  'constraints': {}
              }

              # Critical tables to validate based on our models
              critical_tables = [
                  'texts', 'authors', 'text_divisions', 
                  'lemmas', 'lemma_analyses', 'sentences', 
                  'text_lines', 'lexical_values'
              ]
              
              async with engine.connect() as connection:
                  for table in critical_tables:
                      try:
                          # Check table existence
                          result = await connection.execute(
                              text(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{table}' AND table_schema = 'public')")
                          )
                          table_exists = result.scalar()
                          validation_results['tables'][table] = table_exists

                          if table_exists:
                              # Check table columns and row count
                              columns_result = await connection.execute(
                                  text(f"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '{table}' AND table_schema = 'public'")
                              )
                              columns_count = columns_result.scalar()
                              validation_results['tables'][f'{table}_columns'] = columns_count

                              # Check row count
                              row_count_result = await connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                              row_count = row_count_result.scalar()
                              validation_results['tables'][f'{table}_rows'] = row_count

                      except Exception as e:
                          print(f"‚ùå Error validating table {table}: {e}")
                          validation_results['tables'][table] = False

              return validation_results

          async def performance_baseline(engine):
              performance_metrics = {}
              
              async with engine.connect() as connection:
                  # Measure query performance for critical tables
                  critical_tables = [
                      'texts', 'authors', 'text_divisions', 
                      'lemmas', 'lemma_analyses', 'sentences', 
                      'text_lines', 'lexical_values'
                  ]
                  
                  for table in critical_tables:
                      try:
                          start_time = time.time()
                          result = await connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                          row_count = result.scalar()
                          query_time = time.time() - start_time
                          
                          performance_metrics[table] = {
                              'row_count': row_count,
                              'query_time_ms': query_time * 1000
                          }
                      except Exception as e:
                          print(f"‚ö†Ô∏è Performance check failed for {table}: {e}")
                          performance_metrics[table] = {'error': str(e)}

              return performance_metrics

          async def validate_database(engine, database_name):
              try:
                  # Schema Validation
                  schema_validation = await validate_database_schema(engine)
                  print("üìã Schema Validation Results:")
                  for category, results in schema_validation.items():
                      print(f"{category.capitalize()}: {results}")

                  # Performance Baseline
                  performance_results = await performance_baseline(engine)
                  print("\n‚è±Ô∏è Performance Baseline:")
                  for table, metrics in performance_results.items():
                      print(f"{table}: {metrics}")

                  # Comprehensive Validation Checks
                  async with engine.connect() as connection:
                      # Total table count
                      result = await connection.execute(
                          text("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
                      )
                      table_count = result.scalar()
                      print(f"\n‚úÖ Database '{database_name}' contains {table_count} tables")

                  # Validate critical tables exist and have data
                  critical_tables_valid = all(
                      schema_validation['tables'].get(table, False) 
                      for table in [
                          'texts', 'authors', 'text_divisions', 
                          'lemmas', 'sentences', 'text_lines'
                      ]
                  )

                  performance_valid = all(
                      metrics.get('row_count', 0) > 0 
                      for metrics in performance_results.values()
                  )

                  return critical_tables_valid and performance_valid

              except Exception as e:
                  print(f"‚ùå Database validation failed: {e}")
                  return False

          # Create async database engine
          engine = create_async_engine(
              os.environ['DATABASE_URL'],
              echo=False
          )

          # Run async validation
          async def run_validation():
              try:
                  validation_result = await validate_database(engine, os.environ['POSTGRES_DB'])
                  if validation_result:
                      print("‚úÖ Database restoration validated successfully")
                      return True
                  else:
                      print("‚ùå Database validation failed")
                      return False
              finally:
                  await engine.dispose()

          # Use asyncio to run the async function
          result = asyncio.run(run_validation())
          
          if not result:
              exit(1)
          EOF

  # Remaining workflow steps stay the same...
