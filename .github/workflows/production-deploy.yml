name: Production Deployment Workflow

on:
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Force deployment without Docker Build workflow'
        required: false
        type: boolean
        default: false
  workflow_run:
    workflows: ["Docker Build and Registry Push"]
    types:
      - completed
    branches:
      - production

# Comprehensive permissions for GitHub Actions workflow
permissions:
  contents: read
  packages: read
  id-token: write

env:
  BACKEND_IMAGE_NAME: ${{ github.repository }}-backend
  FRONTEND_IMAGE_NAME: ${{ github.repository }}-frontend

jobs:
  validate-workflow-trigger:
    runs-on: ubuntu-latest
    steps:
      - name: Validate Workflow Trigger
        run: |
          echo "üîç Workflow Trigger Analysis:"
          echo "Event Name: ${{ github.event_name }}"
          echo "Workflow Run Conclusion: ${{ github.event.workflow_run.conclusion }}"
          echo "Workflow Run Name: ${{ github.event.workflow_run.name }}"
          echo "Workflow Run Branch: ${{ github.event.workflow_run.head_branch }}"

  database-restoration:
    needs: validate-workflow-trigger
    uses: ./.github/workflows/database-restoration.yml
    secrets: inherit
    with:
      deployment_mode: production

  database-validation:
    needs: database-restoration
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Retrieve Database Secrets
        id: get-database-secrets
        run: |
          secrets=$(aws secretsmanager get-secret-value --secret-id amta-production-secrets --query SecretString --output text)
          
          database_url=$(echo "$secrets" | jq -r '.DATABASE_URL')
          postgres_host=$(echo "$secrets" | jq -r '.POSTGRES_HOST')
          postgres_port=$(echo "$secrets" | jq -r '.POSTGRES_PORT')
          postgres_db=$(echo "$secrets" | jq -r '.POSTGRES_DB')
          postgres_user=$(echo "$secrets" | jq -r '.POSTGRES_USER')
          
          echo "DATABASE_URL=$database_url" >> $GITHUB_OUTPUT
          echo "POSTGRES_HOST=$postgres_host" >> $GITHUB_OUTPUT
          echo "POSTGRES_PORT=$postgres_port" >> $GITHUB_OUTPUT
          echo "POSTGRES_DB=$postgres_db" >> $GITHUB_OUTPUT
          echo "POSTGRES_USER=$postgres_user" >> $GITHUB_OUTPUT

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install sqlalchemy[asyncio] asyncpg psycopg2-binary greenlet

      - name: Validate Restored Database
        env:
          DATABASE_URL: ${{ steps.get-database-secrets.outputs.DATABASE_URL }}
          POSTGRES_HOST: ${{ steps.get-database-secrets.outputs.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ steps.get-database-secrets.outputs.POSTGRES_PORT }}
          POSTGRES_DB: ${{ steps.get-database-secrets.outputs.POSTGRES_DB }}
          POSTGRES_USER: ${{ steps.get-database-secrets.outputs.POSTGRES_USER }}
        run: |
          python3 - << EOF
          import os
          import asyncio
          import time
          from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
          from sqlalchemy.orm import sessionmaker
          from sqlalchemy import text
          import asyncpg

          async def validate_database_schema(engine):
              validation_results = {
                  'tables': {},
                  'indexes': {},
                  'constraints': {}
              }

              # Critical tables to validate based on our models
              critical_tables = [
                  'texts', 'authors', 'text_divisions', 
                  'lemmas', 'lemma_analyses', 'sentences', 
                  'text_lines', 'lexical_values'
              ]
              
              async with engine.connect() as connection:
                  for table in critical_tables:
                      try:
                          # Check table existence
                          result = await connection.execute(
                              text(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{table}' AND table_schema = 'public')")
                          )
                          table_exists = result.scalar()
                          validation_results['tables'][table] = table_exists

                          if table_exists:
                              # Check table columns and row count
                              columns_result = await connection.execute(
                                  text(f"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '{table}' AND table_schema = 'public'")
                              )
                              columns_count = columns_result.scalar()
                              validation_results['tables'][f'{table}_columns'] = columns_count

                              # Check row count
                              row_count_result = await connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                              row_count = row_count_result.scalar()
                              validation_results['tables'][f'{table}_rows'] = row_count

                      except Exception as e:
                          print(f"‚ùå Error validating table {table}: {e}")
                          validation_results['tables'][table] = False

              return validation_results

          async def performance_baseline(engine):
              performance_metrics = {}
              
              async with engine.connect() as connection:
                  # Measure query performance for critical tables
                  critical_tables = [
                      'texts', 'authors', 'text_divisions', 
                      'lemmas', 'lemma_analyses', 'sentences', 
                      'text_lines', 'lexical_values'
                  ]
                  
                  for table in critical_tables:
                      try:
                          start_time = time.time()
                          result = await connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                          row_count = result.scalar()
                          query_time = time.time() - start_time
                          
                          performance_metrics[table] = {
                              'row_count': row_count,
                              'query_time_ms': query_time * 1000
                          }
                      except Exception as e:
                          print(f"‚ö†Ô∏è Performance check failed for {table}: {e}")
                          performance_metrics[table] = {'error': str(e)}

              return performance_metrics

          async def validate_database(engine, database_name):
              try:
                  # Schema Validation
                  schema_validation = await validate_database_schema(engine)
                  print("üìã Schema Validation Results:")
                  for category, results in schema_validation.items():
                      print(f"{category.capitalize()}: {results}")

                  # Performance Baseline
                  performance_results = await performance_baseline(engine)
                  print("\n‚è±Ô∏è Performance Baseline:")
                  for table, metrics in performance_results.items():
                      print(f"{table}: {metrics}")

                  # Comprehensive Validation Checks
                  async with engine.connect() as connection:
                      # Total table count
                      result = await connection.execute(
                          text("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
                      )
                      table_count = result.scalar()
                      print(f"\n‚úÖ Database '{database_name}' contains {table_count} tables")

                  # Validate critical tables exist and have data
                  critical_tables_valid = all(
                      schema_validation['tables'].get(table, False) 
                      for table in [
                          'texts', 'authors', 'text_divisions', 
                          'lemmas', 'sentences', 'text_lines'
                      ]
                  )

                  performance_valid = all(
                      metrics.get('row_count', 0) > 0 
                      for metrics in performance_results.values()
                  )

                  return critical_tables_valid and performance_valid

              except Exception as e:
                  print(f"‚ùå Database validation failed: {e}")
                  return False

          # Create async database engine
          engine = create_async_engine(
              os.environ['DATABASE_URL'],
              echo=False
          )

          # Run async validation
          async def run_validation():
              try:
                  validation_result = await validate_database(engine, os.environ['POSTGRES_DB'])
                  if validation_result:
                      print("‚úÖ Database restoration validated successfully")
                      return True
                  else:
                      print("‚ùå Database validation failed")
                      return False
              finally:
                  await engine.dispose()

          # Use asyncio to run the async function
          result = asyncio.run(run_validation())
          
          if not result:
              exit(1)
          EOF

  # Deployment Readiness Verification
  deployment-readiness:
    needs: database-validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: production
          fetch-depth: 0

      - name: Set up deployment environment
        run: |
          chmod +x deployment_readiness.sh verify_deployment.sh

      - name: Run Deployment Readiness Checks
        run: |
          ./deployment_readiness.sh production
        env:
          DEPLOYMENT_MODE: production

  # Remaining workflow steps stay the same as in the previous version
  deploy-to-ec2:
    needs: deployment-readiness
    if: |
      github.event_name == 'workflow_dispatch' || 
      (github.event.workflow_run.conclusion == 'success' && 
      github.event.workflow_run.name == 'Docker Build and Registry Push' && 
      contains(github.event.workflow_run.head_branch, 'production'))
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: production
          fetch-depth: 0

      - name: Prepare Deployment Logs
        run: |
          mkdir -p deployment_logs
          chmod 777 deployment_logs
          echo "Workflow initiated at $(date)" > deployment_logs/workflow_start.log
          echo "Repository: ${{ github.repository }}" >> deployment_logs/workflow_start.log
          echo "Workflow Run ID: ${{ github.run_id }}" >> deployment_logs/workflow_start.log
          ls -la deployment_logs

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull Backend Docker Image
        run: |
          BACKEND_REPO_LOWER=$(echo "${{ env.BACKEND_IMAGE_NAME }}" | tr '[:upper:]' '[:lower:]')
          docker pull ghcr.io/$BACKEND_REPO_LOWER:production || true

      - name: Pull Frontend Docker Image
        run: |
          FRONTEND_REPO_LOWER=$(echo "${{ env.FRONTEND_IMAGE_NAME }}" | tr '[:upper:]' '[:lower:]')
          docker pull ghcr.io/$FRONTEND_REPO_LOWER:production || true

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: us-east-1

      - name: Retrieve Production Secrets
        id: get-secrets
        run: |
          secrets=$(aws secretsmanager get-secret-value --secret-id amta-production-secrets --query SecretString --output text)
          
          redis_url=$(echo $secrets | jq -r '.REDIS_URL')
          bedrock_model_id=$(echo $secrets | jq -r '.BEDROCK_MODEL_ID')
          
          echo "::add-mask::$redis_url"
          echo "::add-mask::$bedrock_model_id"
          
          echo "REDIS_URL=$redis_url" >> $GITHUB_OUTPUT
          echo "BEDROCK_MODEL_ID=$bedrock_model_id" >> $GITHUB_OUTPUT

      - name: Prepare Deployment Configuration
        env:
          REDIS_URL: ${{ steps.get-secrets.outputs.REDIS_URL }}
          BEDROCK_MODEL_ID: ${{ steps.get-secrets.outputs.BEDROCK_MODEL_ID }}
        run: |
          echo "DEPLOYMENT_MODE=production" > .env
          echo "REDIS_URL=$REDIS_URL" >> .env
          echo "BEDROCK_MODEL_ID=$BEDROCK_MODEL_ID" >> .env
          echo "AWS_REGION=us-east-1" >> .env
          chmod 600 .env

      - name: Deploy to EC2
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
        run: |
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key

          SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no"

          # Setup EC2 environment
          ssh $SSH_OPTS -i ~/.ssh/ec2_key $EC2_USER@$EC2_HOST << SETUP_EOF
            HOME_DIR=\$(eval echo ~$EC2_USER)
            mkdir -p \$HOME_DIR/atlomy_chat/logs
            chmod 750 \$HOME_DIR/atlomy_chat/logs
            echo "Deployment initiated at \$(date)" > \$HOME_DIR/atlomy_chat/logs/deployment.log
          SETUP_EOF

          # Copy deployment files
          scp $SSH_OPTS -i ~/.ssh/ec2_key \
              docker-compose.yml .env \
              $EC2_USER@$EC2_HOST:~/atlomy_chat/

          # Deploy services
          ssh $SSH_OPTS -i ~/.ssh/ec2_key $EC2_USER@$EC2_HOST << DEPLOY_EOF
            HOME_DIR=\$(eval echo ~$EC2_USER)
            cd \$HOME_DIR/atlomy_chat
            
            # Pull latest images
            docker pull ghcr.io/$(echo "${{ env.BACKEND_IMAGE_NAME }}" | tr '[:upper:]' '[:lower:]'):production
            docker pull ghcr.io/$(echo "${{ env.FRONTEND_IMAGE_NAME }}" | tr '[:upper:]' '[:lower:]'):production
            
            # Stop and remove existing containers
            docker-compose down || true
            
            # Start new deployment
            docker-compose up -d
            
            # Log container status
            docker-compose ps
            docker-compose logs --tail=100
          DEPLOY_EOF

      - name: Verify Deployment
        run: |
          # Run comprehensive deployment verification
          ./verify_deployment.sh

      - name: Verify Deployment Health
        env:
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
        run: |
          SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no"
          
          ssh $SSH_OPTS -i ~/.ssh/ec2_key $EC2_USER@$EC2_HOST << HEALTH_EOF
            HOME_DIR=\$(eval echo ~$EC2_USER)
            cd \$HOME_DIR/atlomy_chat
            
            max_attempts=3
            attempt=0
            
            while [ \$attempt -lt \$max_attempts ]; do
              echo "Health Check Attempt \$((attempt + 1))"
              
              backend_response=\$(curl -s -w "%{http_code}" http://localhost:8081/health)
              frontend_response=\$(curl -s -w "%{http_code}" http://localhost:3000)
              
              backend_status=\${backend_response: -3}
              frontend_status=\${frontend_response: -3}
              
              if [ "\$backend_status" = "200" ] && [ "\$frontend_status" = "200" ]; then
                echo "Successful Health Check: Both Backend and Frontend are Operational"
                exit 0
              fi
              
              echo "Health Check Failed. Backend Status: \$backend_status, Frontend Status: \$frontend_status"
              docker-compose logs --tail=100
              
              attempt=\$((attempt + 1))
              sleep 15
            done
            
            echo "Critical: Health Checks Failed After \$max_attempts Attempts"
            exit 1
          HEALTH_EOF

      - name: Preserve Deployment Logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: deployment-logs
          path: |
            deployment_logs/
            ${{ github.workspace }}/deployment_logs/
          retention-days: 7

      - name: Generate Fallback Logs
        if: failure()
        run: |
          mkdir -p deployment_logs
          echo "Workflow Failed" > deployment_logs/workflow_failure.log
          echo "Workflow Run ID: ${{ github.run_id }}" >> deployment_logs/workflow_failure.log
          echo "Repository: ${{ github.repository }}" >> deployment_logs/workflow_failure.log
          env >> deployment_logs/environment_vars.log

  notify-status:
    if: always()
    needs: [deploy-to-ec2]
    runs-on: ubuntu-latest
    steps:
      - name: Determine Workflow Status
        run: |
          if [[ "${{ needs.deploy-to-ec2.result }}" == "success" ]]; then
            echo "‚úÖ Production Deployment Completed Successfully"
          else
            echo "‚ùå Production Deployment Failed"
            exit 1
          fi