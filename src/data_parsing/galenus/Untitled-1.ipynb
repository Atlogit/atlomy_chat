{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy_transformers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#from LOCAL_SETTINGS import SPACY_MODEL_PATH as spaCy_Model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy_transformers'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "from tqdm import tqdm\n",
    "#from LOCAL_SETTINGS import SPACY_MODEL_PATH as spaCy_Model\n",
    "spaCy_Model = \"models/ner_pipeline_22_dec_trf/model-best\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencizer(text):\n",
    "\n",
    "\n",
    "    delimiters_pattern = r'[.|·]'\n",
    "    sentences = re.split(delimiters_pattern, text)\n",
    "    #print(\"sentence: \", sentences)\n",
    "    return sentences\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "\n",
    "\n",
    "    apostrophes = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\"]  # all possible apostrophes\n",
    "    for apostrophe in apostrophes:\n",
    "        text = text.replace(apostrophe, \"ʼ\")\n",
    "    clean = ' '.join(text.replace('-\\n', '').replace('\\r', ' ').replace('\\n', ' ').split())\n",
    "    #clean = text.replace('-\\n', \"\").replace('\\r', \" \").replace('\\n', \" \")\n",
    "    print(\"clean sentence: \",clean)\n",
    "    return clean\n",
    "def write_to_jsonl(data_list, file_path=\"output.jsonl\"):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for data_dict in data_list:\n",
    "            json.dump(data_dict, file, ensure_ascii=False)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "def create_text_tagging_object(sentences):\n",
    "    nlp = spacy.load(spaCy_Model)\n",
    "\n",
    "    sentences_tagged = []\n",
    "    for sentence in tqdm(sentences, desc=\"Processing sentences\", unit=\"sentence\"):\n",
    "        # Tokenization and part-of-speech tagging\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        doc_dict = {\n",
    "            \"text\": sentence,\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"text\": token.text,\n",
    "                    \"lemma\": token.lemma_,\n",
    "                    \"pos\": token.pos_,\n",
    "                    \"tag\": token.tag_,\n",
    "                    \"category\": token.ent_type_,\n",
    "                    # \"dep\": token.dep_,\n",
    "                    # \"is_alpha\": token.is_alpha,\n",
    "                    # \"is_stop\": token.is_stop,\n",
    "                }\n",
    "                for token in doc\n",
    "            ]\n",
    "        }\n",
    "        sentences_tagged += [doc_dict]\n",
    "    return sentences_tagged\n",
    "\n",
    "def read_jsonl_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_list = [json.loads(line) for line in file]\n",
    "    return data_list\n",
    "def create_data():\n",
    "    galenus_text = ''\n",
    "    with open(\"../assets/texts/TLG0057_galen.txt\", 'r') as file:\n",
    "        galenus_text = file.read()\n",
    "    galenus_text = clean_text(galenus_text)\n",
    "    galenus_sentences = sentencizer(galenus_text)\n",
    "    tagged = create_text_tagging_object(galenus_sentences)\n",
    "    write_to_jsonl(tagged, \"galenus_tagged_data.jsonl\")\n",
    "if __name__ == \"__main__\":\n",
    "    create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbluesearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retrieve_sentences_from_sentence_ids\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "\"\"\"Code to reproduce experiment.\"\"\"\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import sqlalchemy\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from bluesearch.sql import retrieve_sentences_from_sentence_ids\n",
    "\n",
    "mp.set_start_method('fork')  # for n_process is bigger than 1 \n",
    "\n",
    "# 1. Initiate spacy model\n",
    "model = \"en_core_web_trf\"          # or \"en_core_web_lg\"\n",
    "pipes_to_disable = []              # or ['tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
    "entity_ruler = True                # or False\n",
    "DB_URL = \"\"                        # URL to the database\n",
    "\n",
    "spacy.require_gpu()                # or spacy.require_cpu()\n",
    "nlp = spacy.load(model, disable=pipes_to_disable)\n",
    "if entity_ruler:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns([{\"label\": \"LOCATION\", \"pattern\": \"Berlin\"}])\n",
    "\n",
    "# 2. First experiment: nlp(sentence)\n",
    "# 2.a. Create sentence to process\n",
    "n_times = 1\n",
    "sentence = \"Emmanuel Macron is living in Paris.\" * n_times\n",
    "# 2.b. Results\n",
    "doc = nlp(sentence)\n",
    "print(doc.ents)\n",
    "# 2.b. Compute runtime\n",
    "time_results = timeit.repeat(lambda: nlp(sentence), repeat=10, number=10)\n",
    "mean_time = np.mean(time_results)\n",
    "print(mean_time)\n",
    "\n",
    "# 3. Second experiment: nlp.pipe(texts)\n",
    "# 3.a. Load sentences from database\n",
    "engine = sqlalchemy.create_engine(DB_URL)\n",
    "n_sentences = 10\n",
    "n_process = 1 \n",
    "sentences = retrieve_sentences_from_sentence_ids(np.arange(1, n_sentences+1), engine)\n",
    "texts = [(sent[\"text\"], {\"article_id\": sent[\"article_id\"], \"sentence_id\": sent[\"sentence_id\"]}) for i, sent in sentences.iterrows()]\n",
    "all_texts.append(texts)\n",
    "# 3.b. Compute runtime\n",
    "time_results = timeit.repeat(lambda: list(nlp.pipe(texts, as_tuples=True)), repeat=10, number=10)\n",
    "mean_time = np.mean(time_results)\n",
    "print(mean_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlomy_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
